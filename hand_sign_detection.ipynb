{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # I. Création de la base de donées annotées"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.1. Capture des photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from cvzone import HandTrackingModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changer le label en fonction du dataset\n",
    "label = input(\"Entree le label : \")\n",
    "cnt_img = 100\n",
    "detector = HandTrackingModule.HandDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialisation de la webcam\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    #capture d'une image du flux de la webcam\n",
    "    ret,img = capture.read()\n",
    "    img_copy = img.copy()\n",
    "    hands, img = detector.findHands(img)\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Erreur lors de la lecture de img\")\n",
    "        break\n",
    "\n",
    "    #Affichage de l'image\n",
    "    cv2.imshow(\"Image\", img)\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    # ESC \n",
    "    if key%256 == 27:\n",
    "        print(\"ESC, fermeture...\")\n",
    "        break\n",
    "    # ESPACE\n",
    "    elif key%256 == 32:\n",
    "\n",
    "        bbox_value = hands[0].get('bbox')\n",
    "\n",
    "        #Ecrit le roi dans le fichier\n",
    "        roi = img_copy[bbox_value[1]:bbox_value[1] + bbox_value[3], bbox_value[0]:bbox_value[0] + bbox_value[2]]\n",
    "\n",
    "        img_name = \"image_final/{0}/{0}_{1}.png\".format(label,cnt_img)\n",
    "        cv2.imwrite(img_name, roi)\n",
    "        print(\"{} ecrit!\".format(img_name))\n",
    "        cnt_img += 1\n",
    "\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.2. Filtre gaussien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gaussian_filter(img):\n",
    "    # Charger l'image\n",
    "    image = cv2.imread(img)\n",
    "    # Appliquer le filtre Gaussien\n",
    "    blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    # Enregistrer l'image filtrée\n",
    "    cv2.imwrite(img, blurred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_dir = 'image_final'\n",
    "for sub_dir in os.listdir(root_dir):\n",
    "    sub_dir_path = os.path.join(root_dir, sub_dir)\n",
    "    for filename in os.listdir(sub_dir_path):\n",
    "        img = os.path.join(sub_dir_path, filename)\n",
    "        gaussian_filter(img)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.2. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Augmentor\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'image_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 91 image(s) found.\n",
      "Output directory set to image_final\\A\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=127x90 at 0x1B05F621780>: 100%|██████████| 500/500 [00:02<00:00, 177.37 Samples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 79 image(s) found.\n",
      "Output directory set to image_final\\B\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=89x115 at 0x1B05F4918A0>: 100%|██████████| 500/500 [00:01<00:00, 265.12 Samples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 84 image(s) found.\n",
      "Output directory set to image_final\\C\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=118x108 at 0x1B05F66FA00>: 100%|██████████| 500/500 [00:02<00:00, 213.80 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 66 image(s) found.\n",
      "Output directory set to image_final\\G\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=78x132 at 0x1B05F477040>: 100%|██████████| 500/500 [00:02<00:00, 227.36 Samples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 62 image(s) found.\n",
      "Output directory set to image_final\\H\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=107x160 at 0x1B05F5EC910>: 100%|██████████| 500/500 [00:02<00:00, 243.67 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 77 image(s) found.\n",
      "Output directory set to image_final\\I\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=56x87 at 0x1B060699B40>: 100%|██████████| 500/500 [00:01<00:00, 275.30 Samples/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 67 image(s) found.\n",
      "Output directory set to image_final\\L\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=115x154 at 0x1B05F622FB0>: 100%|██████████| 500/500 [00:01<00:00, 299.39 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 74 image(s) found.\n",
      "Output directory set to image_final\\R\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=56x90 at 0x1B05F60ABC0>: 100%|██████████| 500/500 [00:01<00:00, 314.75 Samples/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 71 image(s) found.\n",
      "Output directory set to image_final\\V\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=97x215 at 0x1B05F4AE500>: 100%|██████████| 500/500 [00:03<00:00, 141.08 Samples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 71 image(s) found.\n",
      "Output directory set to image_final\\W\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=86x108 at 0x1B05F608E20>: 100%|██████████| 500/500 [00:01<00:00, 304.96 Samples/s] \n"
     ]
    }
   ],
   "source": [
    "for sub_dir in os.listdir(root_dir):\n",
    "    sub_dir_path = os.path.join(root_dir, sub_dir)\n",
    "\n",
    "    p = Augmentor.Pipeline(sub_dir_path)\n",
    "\n",
    "    p.zoom(probability=0.3,min_factor=0.8,max_factor=1.5)\n",
    "    p.flip_top_bottom(probability=0.4)\n",
    "    p.random_brightness(probability=0.3,min_factor=0.3,max_factor=1.2)\n",
    "    p.random_distortion(probability=1,grid_width=4,grid_height=4,magnitude=8)\n",
    "\n",
    "    p.sample(500)\n",
    "\n",
    "    output_dir = os.path.join(sub_dir_path, 'output')\n",
    "    if os.path.isdir(output_dir):\n",
    "        for filename in os.listdir(output_dir):\n",
    "            src = os.path.join(output_dir, filename)\n",
    "            dst = os.path.join(sub_dir_path, filename)\n",
    "            os.rename(src, dst)\n",
    "        os.rmdir(output_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Création et entrainement d'un réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import cv2\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5742, 50, 50, 3) (5742,)\n"
     ]
    }
   ],
   "source": [
    "# Chargement du dataset X->images y-> labels\n",
    "data_dir = \"image_final/\"\n",
    "labels = sorted(os.listdir(data_dir))\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for idx, label in enumerate(labels):\n",
    "  for file in os.listdir(data_dir + '/'+label):\n",
    "    filepath = data_dir +'/'+ label + \"/\" + file\n",
    "    img = cv2.resize(cv2.imread(filepath),(50,50))\n",
    "    X.append(img)\n",
    "    y.append(idx)\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decoupage du dataset 80% training, 20% test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalisation des pixels et transformation en vecteur binaire\n",
    "Y_train = to_categorical(Y_train)\n",
    "Y_test = to_categorical(Y_test)\n",
    "X_train = X_train / 255.\n",
    "X_test = X_test/ 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 16)        448       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 46, 46, 16)        2320      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 44, 44, 16)        2320      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 22, 22, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 20, 20, 32)        4640      \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 18, 18, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 16, 16, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 6, 6, 64)          18496     \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 2, 2, 64)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 154,762\n",
      "Trainable params: 154,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Création du model\n",
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(16, (3,3), activation ='relu', input_shape=(50,50,3)),\n",
    "        tf.keras.layers.Conv2D(16, (3,3), activation ='relu'),\n",
    "        tf.keras.layers.Conv2D(16, (3,3), activation ='relu'),\n",
    "        tf.keras.layers.MaxPool2D((2,2)),\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation ='relu'),\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation ='relu'),\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation ='relu'),\n",
    "        tf.keras.layers.MaxPool2D((2,2)),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation ='relu'),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation ='relu'),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation ='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "#Compilation \n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "               loss = 'categorical_crossentropy',\n",
    "               metrics=['accuracy'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/36\n",
      "36/36 [==============================] - 8s 135ms/step - loss: 2.3035 - accuracy: 0.0967 - val_loss: 2.3020 - val_accuracy: 0.1012\n",
      "Epoch 2/36\n",
      "36/36 [==============================] - 4s 125ms/step - loss: 2.3055 - accuracy: 0.1098 - val_loss: 2.2971 - val_accuracy: 0.1119\n",
      "Epoch 3/36\n",
      "36/36 [==============================] - 4s 125ms/step - loss: 2.2381 - accuracy: 0.1629 - val_loss: 2.0903 - val_accuracy: 0.1959\n",
      "Epoch 4/36\n",
      "36/36 [==============================] - 4s 126ms/step - loss: 2.0032 - accuracy: 0.2779 - val_loss: 1.8387 - val_accuracy: 0.3167\n",
      "Epoch 5/36\n",
      "36/36 [==============================] - 5s 129ms/step - loss: 1.7085 - accuracy: 0.4007 - val_loss: 1.6678 - val_accuracy: 0.4462\n",
      "Epoch 6/36\n",
      "36/36 [==============================] - 5s 130ms/step - loss: 1.4491 - accuracy: 0.4948 - val_loss: 1.2334 - val_accuracy: 0.5805\n",
      "Epoch 7/36\n",
      "36/36 [==============================] - 5s 130ms/step - loss: 1.0758 - accuracy: 0.6132 - val_loss: 1.0549 - val_accuracy: 0.6448\n",
      "Epoch 8/36\n",
      "36/36 [==============================] - 5s 128ms/step - loss: 0.8273 - accuracy: 0.7326 - val_loss: 0.8283 - val_accuracy: 0.7421\n",
      "Epoch 9/36\n",
      "36/36 [==============================] - 4s 126ms/step - loss: 0.6603 - accuracy: 0.7648 - val_loss: 0.8813 - val_accuracy: 0.7392\n",
      "Epoch 10/36\n",
      "36/36 [==============================] - 4s 124ms/step - loss: 0.5596 - accuracy: 0.8057 - val_loss: 0.7601 - val_accuracy: 0.7645\n",
      "Epoch 11/36\n",
      "36/36 [==============================] - 5s 127ms/step - loss: 0.4422 - accuracy: 0.8458 - val_loss: 0.9913 - val_accuracy: 0.7296\n",
      "Epoch 12/36\n",
      "36/36 [==============================] - 5s 128ms/step - loss: 0.4468 - accuracy: 0.8467 - val_loss: 0.7967 - val_accuracy: 0.7643\n",
      "Epoch 13/36\n",
      "36/36 [==============================] - 5s 128ms/step - loss: 0.4065 - accuracy: 0.8798 - val_loss: 0.8121 - val_accuracy: 0.7980\n",
      "Epoch 14/36\n",
      "36/36 [==============================] - 5s 129ms/step - loss: 0.3260 - accuracy: 0.8894 - val_loss: 0.7677 - val_accuracy: 0.7952\n",
      "Epoch 15/36\n",
      "36/36 [==============================] - 4s 123ms/step - loss: 0.2503 - accuracy: 0.9103 - val_loss: 0.6444 - val_accuracy: 0.8182\n",
      "Epoch 16/36\n",
      "36/36 [==============================] - 5s 132ms/step - loss: 0.1908 - accuracy: 0.9443 - val_loss: 0.7465 - val_accuracy: 0.8137\n",
      "Epoch 17/36\n",
      "36/36 [==============================] - 5s 135ms/step - loss: 0.1517 - accuracy: 0.9556 - val_loss: 0.7102 - val_accuracy: 0.8254\n",
      "Epoch 18/36\n",
      "36/36 [==============================] - 5s 130ms/step - loss: 0.1503 - accuracy: 0.9521 - val_loss: 0.6580 - val_accuracy: 0.8302\n",
      "Epoch 19/36\n",
      "36/36 [==============================] - 4s 126ms/step - loss: 0.0893 - accuracy: 0.9721 - val_loss: 0.7736 - val_accuracy: 0.8361\n",
      "Epoch 20/36\n",
      "36/36 [==============================] - 4s 126ms/step - loss: 0.0933 - accuracy: 0.9704 - val_loss: 0.7400 - val_accuracy: 0.8531\n",
      "Epoch 21/36\n",
      "36/36 [==============================] - 4s 124ms/step - loss: 0.1032 - accuracy: 0.9678 - val_loss: 0.7513 - val_accuracy: 0.8404\n",
      "Epoch 22/36\n",
      "36/36 [==============================] - 5s 128ms/step - loss: 0.0669 - accuracy: 0.9791 - val_loss: 1.0101 - val_accuracy: 0.8121\n",
      "Epoch 23/36\n",
      "36/36 [==============================] - 5s 128ms/step - loss: 0.1904 - accuracy: 0.9512 - val_loss: 0.7228 - val_accuracy: 0.8163\n",
      "Epoch 24/36\n",
      "36/36 [==============================] - 5s 128ms/step - loss: 0.1412 - accuracy: 0.9556 - val_loss: 0.7250 - val_accuracy: 0.8374\n",
      "Epoch 25/36\n",
      "36/36 [==============================] - 5s 127ms/step - loss: 0.0541 - accuracy: 0.9817 - val_loss: 0.8662 - val_accuracy: 0.8431\n",
      "Epoch 26/36\n",
      "36/36 [==============================] - 4s 126ms/step - loss: 0.0625 - accuracy: 0.9826 - val_loss: 0.8788 - val_accuracy: 0.8559\n",
      "Epoch 27/36\n",
      "36/36 [==============================] - 5s 128ms/step - loss: 0.1092 - accuracy: 0.9686 - val_loss: 0.8937 - val_accuracy: 0.8311\n",
      "Epoch 28/36\n",
      "36/36 [==============================] - 5s 129ms/step - loss: 0.0461 - accuracy: 0.9852 - val_loss: 1.0251 - val_accuracy: 0.8376\n",
      "Epoch 29/36\n",
      "36/36 [==============================] - 5s 130ms/step - loss: 0.1071 - accuracy: 0.9765 - val_loss: 0.8472 - val_accuracy: 0.8354\n",
      "Epoch 30/36\n",
      "36/36 [==============================] - 4s 126ms/step - loss: 0.0893 - accuracy: 0.9695 - val_loss: 0.8918 - val_accuracy: 0.8470\n",
      "Epoch 31/36\n",
      "36/36 [==============================] - 4s 126ms/step - loss: 0.0407 - accuracy: 0.9878 - val_loss: 0.9358 - val_accuracy: 0.8552\n",
      "Epoch 32/36\n",
      "36/36 [==============================] - 4s 125ms/step - loss: 0.0082 - accuracy: 0.9991 - val_loss: 0.8790 - val_accuracy: 0.8705\n",
      "Epoch 33/36\n",
      "36/36 [==============================] - 5s 130ms/step - loss: 0.0142 - accuracy: 0.9974 - val_loss: 0.9440 - val_accuracy: 0.8589\n",
      "Epoch 34/36\n",
      "36/36 [==============================] - 5s 128ms/step - loss: 0.0119 - accuracy: 0.9965 - val_loss: 0.9626 - val_accuracy: 0.8674\n",
      "Epoch 35/36\n",
      "36/36 [==============================] - 4s 124ms/step - loss: 0.0021 - accuracy: 0.9991 - val_loss: 0.9820 - val_accuracy: 0.8735\n",
      "Epoch 36/36\n",
      "36/36 [==============================] - 4s 126ms/step - loss: 3.9395e-04 - accuracy: 1.0000 - val_loss: 1.0052 - val_accuracy: 0.8740\n"
     ]
    }
   ],
   "source": [
    "#Entrainement\n",
    "history = model.fit(X_train, Y_train, epochs=36, verbose=1,\n",
    "                validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_save/model_final.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Détéction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.1. Détéction avec des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 118ms/step\n",
      "B\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "#Chargement du modèle\n",
    "loaded_model = load_model(\"model_save/model_final.h5\")\n",
    "\n",
    "\n",
    "classData = {\n",
    "    0 : \"A\",\n",
    "    1 : \"B\",\n",
    "    2 : \"C\",\n",
    "    3 : \"G\",\n",
    "    4 : \"H\",\n",
    "    5 : \"I\",\n",
    "    6 : \"L\",\n",
    "    7 : 'R',\n",
    "    8 : 'V',\n",
    "    9 : 'W'\n",
    "}\n",
    "#Chargement de l'image\n",
    "new_image = cv2.imread(\"img_val/Val_0.png\")\n",
    "\n",
    "\n",
    "#Prétraitements identique aux images d'entraînement\n",
    "new_image = cv2.GaussianBlur(new_image, (5, 5), 0)   \n",
    "new_image = cv2.resize(new_image, (50, 50))\n",
    "new_image = np.array(new_image) / 255.0\n",
    "new_image = np.expand_dims(new_image, axis=0)\n",
    "# Prediction\n",
    "predictions = loaded_model.predict(new_image)\n",
    "# Récupère la classe avec la plus grande probabilité\n",
    "class_index = np.argmax(predictions[0])\n",
    "#Affichage\n",
    "results = predictions[0].tolist()\n",
    "print(classData[class_index])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.2. Détéction avec un flux vidéo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from cvzone import HandTrackingModule\n",
    "\n",
    "\n",
    "#Initialisation de la webcamA\n",
    "capture = cv2.VideoCapture(0)\n",
    "detector = HandTrackingModule.HandDetector()\n",
    "loaded_model = load_model(\"model_save/model_final.h5\")\n",
    "\n",
    "classData = {\n",
    "    0 : \"A\",\n",
    "    1 : \"B\",\n",
    "    2 : \"C\",\n",
    "    3 : \"G\",\n",
    "    4 : \"H\",\n",
    "    5 : \"I\",\n",
    "    6 : \"L\",\n",
    "    7 : 'R',\n",
    "    8 : 'V',\n",
    "    9 : 'W'\n",
    "}\n",
    "while True:\n",
    "    #capture d'une image du flux de la webcam\n",
    "    # ret,img = capture.read()\n",
    "    ret,img = capture.read()\n",
    "    img_copy = img.copy()\n",
    "    hands, img = detector.findHands(img)\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Erreur lors de la lecture de img\")\n",
    "        break\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    # ESC \n",
    "    if key%256 == 27:\n",
    "        print(\"ESC, fermeture...\")\n",
    "        break\n",
    "    # ESPACE\n",
    "    # elif key%256 == 32:\n",
    "    if hands != []:\n",
    "        #Ecrit le roi dans le fichier\n",
    "        bbox_value = hands[0].get('bbox')\n",
    "        new_image = img_copy[bbox_value[1]:bbox_value[1] + bbox_value[3], bbox_value[0]:bbox_value[0] + bbox_value[2]]\n",
    "\n",
    "        #Prétraitements identique aux images d'entraînement\n",
    "        new_image = cv2.GaussianBlur(new_image, (5, 5), 0)\n",
    "        new_image = cv2.resize(new_image, (50, 50))\n",
    "        new_image = np.array(new_image) / 255.0\n",
    "        new_image = np.expand_dims(new_image, axis=0)\n",
    "\n",
    "        #predictions\n",
    "        predictions = loaded_model.predict(new_image)\n",
    "        class_index = np.argmax(predictions[0])\n",
    "\n",
    "        # print(predictions)\n",
    "        results = predictions[0].tolist()\n",
    "        print(results)\n",
    "        print(\"Predicted class index:\", class_index,classData[class_index])\n",
    "        #Affichage du texte\n",
    "        cv2.putText(img, f\"signe : {classData[class_index]} [{(results[class_index]):.2f}%]\", (30, 30), cv2.FONT_HERSHEY_PLAIN,2, (255, 0, 0), 2)\n",
    "        \n",
    "    #Affichage de l'image\n",
    "    cv2.imshow(\"Image\", img)\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "301a17a29b57d3836b7901af1621afd6d2b1f2298b9c7949191147cf2fea93e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
