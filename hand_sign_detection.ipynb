{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # I. Création de la base de donées annotées"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.1. Capture des photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from cvzone import HandTrackingModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changer le label en fonction du dataset\n",
    "label = input(\"Entree le label : \")\n",
    "cnt_img = 100\n",
    "detector = HandTrackingModule.HandDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialisation de la webcam\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    #capture d'une image du flux de la webcam\n",
    "    ret,img = capture.read()\n",
    "    img_copy = img.copy()\n",
    "    hands, img = detector.findHands(img)\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Erreur lors de la lecture de img\")\n",
    "        break\n",
    "\n",
    "    #Affichage de l'image\n",
    "    cv2.imshow(\"Image\", img)\n",
    "\n",
    "    key = cv2.waitKey(1)\n",
    "    # ESC \n",
    "    if key%256 == 27:\n",
    "        print(\"ESC, fermeture...\")\n",
    "        break\n",
    "    # ESPACE\n",
    "    elif key%256 == 32:\n",
    "\n",
    "        bbox_value = hands[0].get('bbox')\n",
    "\n",
    "        #Ecrit le roi dans le fichier\n",
    "        roi = img_copy[bbox_value[1]:bbox_value[1] + bbox_value[3], bbox_value[0]:bbox_value[0] + bbox_value[2]]\n",
    "\n",
    "        img_name = \"image_final/{0}/{0}_{1}.png\".format(label,cnt_img)\n",
    "        cv2.imwrite(img_name, roi)\n",
    "        print(\"{} ecrit!\".format(img_name))\n",
    "        cnt_img += 1\n",
    "\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.2. Gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gaussian_filter(img):\n",
    "    # Charger l'image\n",
    "    image = cv2.imread(img)\n",
    "    # Appliquer le filtre Gaussien\n",
    "    blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "    # Enregistrer l'image filtrée\n",
    "    cv2.imwrite(img, blurred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_dir = 'image_final'\n",
    "for sub_dir in os.listdir(root_dir):\n",
    "    sub_dir_path = os.path.join(root_dir, sub_dir)\n",
    "    for filename in os.listdir(sub_dir_path):\n",
    "        img = os.path.join(sub_dir_path, filename)\n",
    "        gaussian_filter(img)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.2. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Augmentor\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'image_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 91 image(s) found.\n",
      "Output directory set to image_final\\A\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=137x189 at 0x1DA5D79BA30>: 100%|██████████| 500/500 [00:03<00:00, 151.80 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 79 image(s) found.\n",
      "Output directory set to image_final\\B\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=85x200 at 0x1DA5D7B7FA0>: 100%|██████████| 500/500 [00:02<00:00, 190.07 Samples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 84 image(s) found.\n",
      "Output directory set to image_final\\C\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=119x109 at 0x1DA5D5E6710>: 100%|██████████| 500/500 [00:02<00:00, 213.66 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 66 image(s) found.\n",
      "Output directory set to image_final\\G\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=89x164 at 0x1DA5D810460>: 100%|██████████| 500/500 [00:02<00:00, 203.10 Samples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 62 image(s) found.\n",
      "Output directory set to image_final\\H\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=128x241 at 0x1DA5D53CBB0>: 100%|██████████| 500/500 [00:02<00:00, 243.22 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 77 image(s) found.\n",
      "Output directory set to image_final\\I\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=299x386 at 0x1DA5D8106A0>: 100%|██████████| 500/500 [00:01<00:00, 251.60 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 67 image(s) found.\n",
      "Output directory set to image_final\\L\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=79x85 at 0x1DA5D7C0A90>: 100%|██████████| 500/500 [00:01<00:00, 278.06 Samples/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 74 image(s) found.\n",
      "Output directory set to image_final\\R\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=47x94 at 0x1DA5D77CD90>: 100%|██████████| 500/500 [00:02<00:00, 186.40 Samples/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 71 image(s) found.\n",
      "Output directory set to image_final\\V\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=99x207 at 0x1DA5E8145E0>: 100%|██████████| 500/500 [00:02<00:00, 190.39 Samples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 71 image(s) found.\n",
      "Output directory set to image_final\\W\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=72x89 at 0x1DA5D7626E0>: 100%|██████████| 500/500 [00:01<00:00, 263.21 Samples/s]  \n"
     ]
    }
   ],
   "source": [
    "for sub_dir in os.listdir(root_dir):\n",
    "    sub_dir_path = os.path.join(root_dir, sub_dir)\n",
    "\n",
    "    p = Augmentor.Pipeline(sub_dir_path)\n",
    "\n",
    "    p.zoom(probability=0.3,min_factor=0.8,max_factor=1.5)\n",
    "    p.flip_top_bottom(probability=0.4)\n",
    "    p.random_brightness(probability=0.3,min_factor=0.3,max_factor=1.2)\n",
    "    p.random_distortion(probability=1,grid_width=4,grid_height=4,magnitude=8)\n",
    "\n",
    "    p.sample(500)\n",
    "\n",
    "    output_dir = os.path.join(sub_dir_path, 'output')\n",
    "    if os.path.isdir(output_dir):\n",
    "        for filename in os.listdir(output_dir):\n",
    "            src = os.path.join(output_dir, filename)\n",
    "            dst = os.path.join(sub_dir_path, filename)\n",
    "            os.rename(src, dst)\n",
    "        os.rmdir(output_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Création et entrainement d'un réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import cv2\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5742, 50, 50, 3) (5742,)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"image_final/\"\n",
    "labels = sorted(os.listdir(data_dir))\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for idx, label in enumerate(labels):\n",
    "  for file in os.listdir(data_dir + '/'+label):\n",
    "    filepath = data_dir +'/'+ label + \"/\" + file\n",
    "    img = cv2.resize(cv2.imread(filepath),(50,50))\n",
    "    X.append(img)\n",
    "    y.append(idx)\n",
    "X = np.asarray(X)\n",
    "y = np.asarray(y)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting dataset into 80% train, 10% validation and 10% test data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.8)\n",
    "X_test, X_eval, Y_test, Y_eval = train_test_split(X_test, Y_test, test_size = 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting Y_tes and Y_train to One hot vectors using to_categorical\n",
    "# example of one hot => '1' is represented as [0. 1. 0. . . . . 0.]\n",
    "Y_train = to_categorical(Y_train)\n",
    "Y_test = to_categorical(Y_test)\n",
    "Y_eval = to_categorical(Y_eval)\n",
    "X_train = X_train / 255.\n",
    "X_test = X_test/ 255.\n",
    "X_eval = X_eval/ 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 16)        448       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 46, 46, 16)        2320      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 44, 44, 16)        2320      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 22, 22, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 20, 20, 32)        4640      \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 18, 18, 32)        9248      \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 16, 16, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 6, 6, 64)          18496     \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 2, 2, 64)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 154,762\n",
      "Trainable params: 154,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# building our model\n",
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(16, (3,3), activation ='relu', input_shape=(50,50,3)),\n",
    "        tf.keras.layers.Conv2D(16, (3,3), activation ='relu'),\n",
    "        tf.keras.layers.Conv2D(16, (3,3), activation ='relu'),\n",
    "        tf.keras.layers.MaxPool2D((2,2)),\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation ='relu'),\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation ='relu'),\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation ='relu'),\n",
    "        tf.keras.layers.MaxPool2D((2,2)),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation ='relu'),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation ='relu'),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation ='relu'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "#compiling the model\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "model.compile(optimizer=optimizer,\n",
    "               loss = 'categorical_crossentropy',\n",
    "               metrics=['accuracy'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/36\n",
      "36/36 [==============================] - 6s 112ms/step - loss: 2.2931 - accuracy: 0.1080 - val_loss: 2.2220 - val_accuracy: 0.1280\n",
      "Epoch 2/36\n",
      "36/36 [==============================] - 4s 105ms/step - loss: 1.9705 - accuracy: 0.2805 - val_loss: 1.6869 - val_accuracy: 0.3944\n",
      "Epoch 3/36\n",
      "36/36 [==============================] - 3s 97ms/step - loss: 1.5137 - accuracy: 0.5009 - val_loss: 1.4456 - val_accuracy: 0.5159\n",
      "Epoch 4/36\n",
      "36/36 [==============================] - 4s 112ms/step - loss: 1.1679 - accuracy: 0.6010 - val_loss: 1.1003 - val_accuracy: 0.6291\n",
      "Epoch 5/36\n",
      "36/36 [==============================] - 4s 104ms/step - loss: 0.9788 - accuracy: 0.6611 - val_loss: 1.0759 - val_accuracy: 0.6452\n",
      "Epoch 6/36\n",
      "36/36 [==============================] - 4s 100ms/step - loss: 0.8171 - accuracy: 0.7361 - val_loss: 0.8969 - val_accuracy: 0.7048\n",
      "Epoch 7/36\n",
      "36/36 [==============================] - 4s 100ms/step - loss: 0.6657 - accuracy: 0.7692 - val_loss: 0.7897 - val_accuracy: 0.7431\n",
      "Epoch 8/36\n",
      "36/36 [==============================] - 4s 103ms/step - loss: 0.5548 - accuracy: 0.8110 - val_loss: 0.7047 - val_accuracy: 0.7867\n",
      "Epoch 9/36\n",
      "36/36 [==============================] - 4s 99ms/step - loss: 0.3942 - accuracy: 0.8737 - val_loss: 0.7437 - val_accuracy: 0.7910\n",
      "Epoch 10/36\n",
      "36/36 [==============================] - 4s 99ms/step - loss: 0.3377 - accuracy: 0.8894 - val_loss: 0.6761 - val_accuracy: 0.8071\n",
      "Epoch 11/36\n",
      "36/36 [==============================] - 4s 103ms/step - loss: 0.2648 - accuracy: 0.9103 - val_loss: 0.6673 - val_accuracy: 0.8176\n",
      "Epoch 12/36\n",
      "36/36 [==============================] - 4s 98ms/step - loss: 0.3066 - accuracy: 0.9033 - val_loss: 0.7820 - val_accuracy: 0.8063\n",
      "Epoch 13/36\n",
      "36/36 [==============================] - 4s 102ms/step - loss: 0.2329 - accuracy: 0.9286 - val_loss: 0.7447 - val_accuracy: 0.8028\n",
      "Epoch 14/36\n",
      "36/36 [==============================] - 4s 102ms/step - loss: 0.2079 - accuracy: 0.9303 - val_loss: 0.7612 - val_accuracy: 0.8111\n",
      "Epoch 15/36\n",
      "36/36 [==============================] - 4s 104ms/step - loss: 0.1998 - accuracy: 0.9312 - val_loss: 0.7349 - val_accuracy: 0.8141\n",
      "Epoch 16/36\n",
      "36/36 [==============================] - 4s 98ms/step - loss: 0.1047 - accuracy: 0.9660 - val_loss: 0.8756 - val_accuracy: 0.8010\n",
      "Epoch 17/36\n",
      "36/36 [==============================] - 4s 99ms/step - loss: 0.1979 - accuracy: 0.9364 - val_loss: 0.8286 - val_accuracy: 0.7714\n",
      "Epoch 18/36\n",
      "36/36 [==============================] - 4s 102ms/step - loss: 0.1383 - accuracy: 0.9591 - val_loss: 1.0387 - val_accuracy: 0.8158\n",
      "Epoch 19/36\n",
      "36/36 [==============================] - 4s 101ms/step - loss: 0.1679 - accuracy: 0.9443 - val_loss: 0.7297 - val_accuracy: 0.8350\n",
      "Epoch 20/36\n",
      "36/36 [==============================] - 4s 107ms/step - loss: 0.0640 - accuracy: 0.9887 - val_loss: 1.2356 - val_accuracy: 0.8006\n",
      "Epoch 21/36\n",
      "36/36 [==============================] - 3s 97ms/step - loss: 0.0786 - accuracy: 0.9721 - val_loss: 1.0359 - val_accuracy: 0.8172\n",
      "Epoch 22/36\n",
      "36/36 [==============================] - 3s 97ms/step - loss: 0.1301 - accuracy: 0.9634 - val_loss: 0.8954 - val_accuracy: 0.8206\n",
      "Epoch 23/36\n",
      "36/36 [==============================] - 4s 106ms/step - loss: 0.0341 - accuracy: 0.9878 - val_loss: 0.9791 - val_accuracy: 0.8511\n",
      "Epoch 24/36\n",
      "36/36 [==============================] - 4s 109ms/step - loss: 0.0391 - accuracy: 0.9861 - val_loss: 1.3314 - val_accuracy: 0.7889\n",
      "Epoch 25/36\n",
      "36/36 [==============================] - 4s 102ms/step - loss: 0.1726 - accuracy: 0.9486 - val_loss: 0.8245 - val_accuracy: 0.8041\n",
      "Epoch 26/36\n",
      "36/36 [==============================] - 4s 101ms/step - loss: 0.0445 - accuracy: 0.9852 - val_loss: 1.0149 - val_accuracy: 0.8481\n",
      "Epoch 27/36\n",
      "36/36 [==============================] - 4s 103ms/step - loss: 0.0309 - accuracy: 0.9948 - val_loss: 0.9806 - val_accuracy: 0.8550\n",
      "Epoch 28/36\n",
      "36/36 [==============================] - 4s 101ms/step - loss: 0.0051 - accuracy: 0.9991 - val_loss: 1.1266 - val_accuracy: 0.8611\n",
      "Epoch 29/36\n",
      "36/36 [==============================] - 4s 100ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 1.2029 - val_accuracy: 0.8616\n",
      "Epoch 30/36\n",
      "36/36 [==============================] - 4s 102ms/step - loss: 6.2141e-04 - accuracy: 1.0000 - val_loss: 1.2608 - val_accuracy: 0.8598\n",
      "Epoch 31/36\n",
      "36/36 [==============================] - 4s 100ms/step - loss: 4.1120e-04 - accuracy: 1.0000 - val_loss: 1.2991 - val_accuracy: 0.8611\n",
      "Epoch 32/36\n",
      "36/36 [==============================] - 4s 100ms/step - loss: 3.1724e-04 - accuracy: 1.0000 - val_loss: 1.3388 - val_accuracy: 0.8620\n",
      "Epoch 33/36\n",
      "36/36 [==============================] - 4s 102ms/step - loss: 2.4683e-04 - accuracy: 1.0000 - val_loss: 1.3712 - val_accuracy: 0.8616\n",
      "Epoch 34/36\n",
      "36/36 [==============================] - 4s 101ms/step - loss: 2.0208e-04 - accuracy: 1.0000 - val_loss: 1.3976 - val_accuracy: 0.8611\n",
      "Epoch 35/36\n",
      "36/36 [==============================] - 4s 102ms/step - loss: 1.6917e-04 - accuracy: 1.0000 - val_loss: 1.4211 - val_accuracy: 0.8611\n",
      "Epoch 36/36\n",
      "36/36 [==============================] - 4s 105ms/step - loss: 1.4634e-04 - accuracy: 1.0000 - val_loss: 1.4447 - val_accuracy: 0.8607\n"
     ]
    }
   ],
   "source": [
    "#start training(fitting) the data\n",
    "history = model.fit(X_train, Y_train, epochs=36, verbose=1,\n",
    "                validation_data=(X_eval, Y_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 1s 15ms/step - loss: 1.2901 - accuracy: 0.8698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2901418209075928, 0.8698301911354065]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_save/model_avec_gauss.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Détéction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.1. Détéction avec des images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.2. Détéction avec un flux vidéo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "301a17a29b57d3836b7901af1621afd6d2b1f2298b9c7949191147cf2fea93e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
